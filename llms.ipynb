{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPoEx8Wjo3End7WfddeCZwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Usermer/LLMs/blob/main/llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri0CXaAhhIrv",
        "outputId": "e94e5cf9-d81c-4545-88cf-5479baaa4487"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade ipywidgets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUI-Bt_KxzVs",
        "outputId": "10b2eb06-d795-4a70-a6c7-f0dbe0ded69d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (8.1.8)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (4.0.15)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.0.16)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La bibiliotheque transformers"
      ],
      "metadata": {
        "id": "ip9riKzGhINt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40855f0f"
      },
      "source": [
        "La biblioth√®que ü§ó Transformers de Hugging Face est un outil puissant pour utiliser des mod√®les de transformeurs pr√©-entra√Æn√©s dans le domaine du NLP, de la vision par ordinateur et de l'audio.\n",
        "\n",
        "Points cl√©s :\n",
        "\n",
        "*   Acc√®s √† des milliers de mod√®les pr√©-entra√Æn√©s (BERT, GPT-2, etc.).\n",
        "*   API simples pour charger, utiliser et entra√Æner les mod√®les.\n",
        "*   Supporte de nombreuses t√¢ches (classification de texte, g√©n√©ration, NER, etc.).\n",
        "*   Compatible avec PyTorch, TensorFlow et JAX.\n",
        "*   Permet la personnalisation et l'entra√Ænement de mod√®les.\n",
        "\n",
        "En bref, Transformers simplifie l'utilisation des mod√®les de transformeurs de pointe pour les chercheurs et d√©veloppeurs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline**"
      ],
      "metadata": {
        "id": "lEU14GTenBjQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ed15a1"
      },
      "source": [
        "La fonction `pipeline` de la biblioth√®que ü§ó Transformers est une interface de haut niveau qui simplifie l'utilisation de mod√®les pr√©-entra√Æn√©s pour des t√¢ches sp√©cifiques. Elle g√®re automatiquement de nombreuses √©tapes, telles que le pr√©traitement des donn√©es, le chargement du mod√®le et la post-traitement des r√©sultats.\n",
        "\n",
        "En gros, vous sp√©cifiez la t√¢che que vous souhaitez effectuer (par exemple, \"summarization\", \"text-classification\", \"question-answering\") et `pipeline` charge un mod√®le appropri√© et configure tout ce dont vous avez besoin pour commencer √† l'utiliser imm√©diatement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Summarization***"
      ],
      "metadata": {
        "id": "s4keRheXpuQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"sshleifer/distilbart-cnn-12-6\"  # mod√®le distill√© et rapide\n",
        ")\n",
        "\n",
        "text=\"walking amid Gion's Machiya wooden houses is a mesmerizig experience .the beautifully preserved sturectures exuded an old-world charm that transports visitores back in time , making them fell like they had stepped into a living museum.the glow lanterns lining the narrow streets add tp the enchanting ambiance,making each stroll of memorable journey throug japan's rich cullural history\"\n",
        "\n",
        "summary=summarizer(text,\n",
        "                   max_length=70,\n",
        "                   min_length=30,  # longueur minimale du r√©sum√©\n",
        "                  #  do_sample=False  # pas de g√©n√©ration al√©atoire\n",
        "                   )\n",
        "print(summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC61UgzqkRnp",
        "outputId": "8aa50711-7847-4eaa-a76d-4891c12e6551"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Walking amid Gion's Machiya wooden houses is a mesmerizig experience . The beautifully preserved sturectures exuded an old-world charm that transports visitores back in time .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Text generation***"
      ],
      "metadata": {
        "id": "KiqGi7h8p719"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Cr√©ation du pipeline pour g√©n√©rer du texte\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Texte de d√©part\n",
        "prompt = \"Once upon a time in a small village,\"\n",
        "\n",
        "# G√©n√©ration du texte\n",
        "result = generator(prompt, max_length=100, do_sample=True, temperature=0.8)\n",
        "\n",
        "print(result[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqanyXAup4G-",
        "outputId": "205fce79-7562-442f-bd81-6efea23d9f83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a small village, it was used to deliver food to the dead.\n",
            "\n",
            "In the end, there was one child that stayed in the village for a long time. He used to talk to the villagers about their food supplies, but they didn't like that.\n",
            "\n",
            "He was quite happy when he got used to it.\n",
            "\n",
            "„ÄåI wonder what they all think about it„Äç(Keiichi)\n",
            "\n",
            "„ÄåI don't think so„Äç(Yup‚Ä¶!)\n",
            "\n",
            "‚Ä¶Well, it is true that there will be a lot of dead children, but the villagers will surely take it into consideration.\n",
            "\n",
            "„ÄåWell then, how about the rest of it? Let's go home.\n",
            "\n",
            "„ÄåEeeah, Yuna-sama, I can't resist that„Äç(Yuna)\n",
            "\n",
            "„ÄåYes, I guess you're right. I'm just going to keep going to the village to buy food„Äç(Yuna)\n",
            "\n",
            "Yuna smiled at Keiichi.\n",
            "\n",
            "„ÄåI really am glad that you chose to move to the village„Äç(Keiichi)\n",
            "\n",
            "„ÄåWell then, I really want to go home„Äç(Yuna)\n",
            "\n",
            "„ÄåUh‚Ä¶„Äç(Yuna)\n",
            "\n",
            "„ÄåYou're welcome Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***language translation***"
      ],
      "metadata": {
        "id": "nxIc4JbOu4H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_text = \"Este curso sobre LLMs se est√° poniendo muy interesante\"\n",
        "\n",
        "# Define the pipeline\n",
        "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the Spanish text\n",
        "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z473XhySu_rV",
        "outputId": "113e1638-4045-4afe-db21-a4648f5ea8d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This course on LLMs is getting very interesting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f93fe1d"
      },
      "source": [
        "## Autoclasses dans Transformers\n",
        "\n",
        "Les \"autoclasses\" comme `AutoModel`, `AutoTokenizer`, `AutoConfig`, etc., sont des classes de la biblioth√®que Transformers qui vous permettent de charger automatiquement la bonne architecture de mod√®le, de tokenizer ou de configuration en fonction du nom ou du chemin d'un mod√®le pr√©-entra√Æn√©.\n",
        "\n",
        "Au lieu d'avoir √† sp√©cifier explicitement la classe de mod√®le (par exemple, `BertModel`, `GPT2Tokenizer`), vous pouvez utiliser l'autoclasse correspondante et la biblioth√®que d√©terminera la classe appropri√©e √† charger.\n",
        "\n",
        "Cela rend votre code plus flexible et plus facile √† utiliser avec diff√©rents mod√®les sans avoir √† modifier le code pour chaque nouveau mod√®le.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77cd85c1"
      },
      "source": [
        "Voici un tableau des autoclasses courantes dans la biblioth√®que ü§ó Transformers :\n",
        "\n",
        "| Type          | Autoclasse      | Description                                                                 | Exemple d'utilisation                                                                 |\n",
        "| :------------ | :-------------- | :-------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ |\n",
        "| **Tokenizers** | `AutoTokenizer` | Charge automatiquement le tokenizer appropri√© pour un mod√®le donn√©.           | `tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")`                    |\n",
        "| **Base Models**    | `AutoModel`     | Charge automatiquement l'architecture de mod√®le de base.                      | `model = AutoModel.from_pretrained(\"bert-base-uncased\")`                              |\n",
        "| **Pour la classification** | `AutoModelForSequenceClassification` | Charge un mod√®le avec une t√™te de classification de s√©quence.             | `model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`   |\n",
        "|               | `AutoModelForTokenClassification` | Charge un mod√®le avec une t√™te de classification de tokens (NER).        | `model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`      |\n",
        "|**Pour question-r√©ponse**               | `AutoModelForQuestionAnswering` | Charge un mod√®le avec une t√™te de r√©ponse √† des questions.              | `model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking\")` |\n",
        "|**Pour la g√©n√©ration du texte**               | `AutoModelForCausalLM` | Charge un mod√®le pour la mod√©lisation causale du langage (g√©n√©ration de texte). | `model = AutoModelForCausalLM.from_pretrained(\"gpt2\")`                                |\n",
        "|**Pour le texte √† texte**               | `AutoModelForSeq2SeqLM` | Charge un mod√®le pour les t√¢ches s√©quence √† s√©quence (traduction, r√©sum√©). | `model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")`                           |\n",
        "| **Configurations** | `AutoConfig`    | Charge automatiquement la configuration appropri√©e pour un mod√®le donn√©.    | `config = AutoConfig.from_pretrained(\"bert-base-uncased\")`                            |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***`AutoTokenizer`***\n",
        "\n",
        "Sert √† transformer le texte en tokens num√©riques (et inversement)."
      ],
      "metadata": {
        "id": "HQqCKjWQLxZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text=\"meryem kada\"\n",
        "\n",
        "#convertir le texte en nombres\n",
        "tokens=tokenizer(text)\n",
        "print(\"tokens:\",tokens)#retourne un dictionnaire\n",
        "\n",
        "\n",
        "#convertir les nombres en texte\n",
        "decoded=tokenizer.decode(tokens[\"input_ids\"])\n",
        "print(\"le texte initial:\",decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJH0ZrPAL2Sv",
        "outputId": "802abfe8-136b-47c0-b076-cf3425fddb39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: {'input_ids': [101, 21442, 6672, 2213, 10556, 2850, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
            "le texte initial: [CLS] meryem kada [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`AutoModel`**\n",
        "\n",
        "Charge le mod√®le de base sans couche de classification ni g√©n√©ration.\n",
        "Id√©al pour obtenir des repr√©sentations (embeddings)."
      ],
      "metadata": {
        "id": "ELYv3ATKN16E"
      }
    }
  ]
}